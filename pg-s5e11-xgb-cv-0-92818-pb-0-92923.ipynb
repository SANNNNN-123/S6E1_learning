{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.kaggle.com/competitions/playground-series-s5e11/writeups/1st-place-a-lot-of-features-a-lot-of-models-an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAR-XxxGGs6X",
    "outputId": "8c3a5949-12ab-4b79-e562-3a3eddf0bdd0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T04:37:10.384428Z",
     "iopub.status.busy": "2025-11-23T04:37:10.383887Z",
     "iopub.status.idle": "2025-11-23T04:37:11.448197Z",
     "shell.execute_reply": "2025-11-23T04:37:11.447609Z",
     "shell.execute_reply.started": "2025-11-23T04:37:10.384404Z"
    },
    "id": "BmsnM5q5Gs6Y",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from itertools import combinations\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.special import expit\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "TARGET = 'loan_paid_back'\n",
    "NUMS = ['annual_income', 'debt_to_income_ratio', 'credit_score', 'loan_amount', 'interest_rate']\n",
    "CATS = ['gender', 'marital_status', 'education_level', 'employment_status', 'loan_purpose', 'grade_subgrade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T04:37:24.942782Z",
     "iopub.status.busy": "2025-11-23T04:37:24.94238Z",
     "iopub.status.idle": "2025-11-23T04:40:49.722876Z",
     "shell.execute_reply": "2025-11-23T04:40:49.721959Z",
     "shell.execute_reply.started": "2025-11-23T04:37:24.942753Z"
    },
    "id": "JWlcY4utGs6e",
    "outputId": "4147ec9c-01d2-4068-ba4e-0f23fe617256",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv', index_col='id')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv', index_col='id')\n",
    "orig = pd.read_csv('/kaggle/input/loan-prediction-dataset-2025/loan_dataset_20000.csv')[NUMS + CATS + [TARGET]]\n",
    "\n",
    "bin_features_train = pd.DataFrame(index=train.index)\n",
    "bin_features_test = pd.DataFrame(index=test.index)\n",
    "\n",
    "for c in NUMS:\n",
    "    for q in [5]:\n",
    "        try:\n",
    "            train_bins, bins = pd.qcut(train[c], q=q, labels=False, retbins=True, duplicates=\"drop\")\n",
    "            bin_features_train[f\"{c}_bin{q}\"] = train_bins\n",
    "            bin_features_test[f\"{c}_bin{q}\"] = pd.cut(test[c], bins=bins, labels=False, include_lowest=True)\n",
    "        except Exception:\n",
    "            bin_features_train[f\"{c}_bin{q}\"] = 0\n",
    "            bin_features_test[f\"{c}_bin{q}\"] = 0\n",
    "train = pd.concat([train, bin_features_train], axis=1)\n",
    "test = pd.concat([test, bin_features_test], axis=1)\n",
    "\n",
    "train['default_risk'] = (train['debt_to_income_ratio'] * 0.40 + (850 - train['credit_score']) / 850 * 0.35 + train['interest_rate'] / 100 * 0.25)\n",
    "test['default_risk'] = (test['debt_to_income_ratio'] * 0.40 + (850 - test['credit_score']) / 850 * 0.35 + test['interest_rate'] / 100 * 0.25)\n",
    "orig['default_risk'] = (orig['debt_to_income_ratio'] * 0.40 + (850 - orig['credit_score']) / 850 * 0.35 + orig['interest_rate'] / 100 * 0.25)\n",
    "\n",
    "for c in ['credit_score']:\n",
    "    n = f'{c}2'\n",
    "    train[n] = train[c].copy()\n",
    "    test[n] = test[c].copy()\n",
    "    orig[n] = orig[c].copy()\n",
    "\n",
    "DIGITS = []\n",
    "for c in ['annual_income', 'loan_amount']:\n",
    "    for k in range(-4, 2):\n",
    "        n = f'{c}_d{k}'\n",
    "        train[n] = ((train[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        test[n] = ((test[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        orig[n] = ((orig[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        DIGITS.append(n)\n",
    "\n",
    "for c in ['interest_rate']:\n",
    "    for k in range(-1, 3):\n",
    "        n = f'{c}_d{k}'\n",
    "        train[n] = ((train[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        test[n] = ((test[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        orig[n] = ((orig[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        DIGITS.append(n)\n",
    "\n",
    "for c in ['debt_to_income_ratio']:\n",
    "    for k in range(1, 4):\n",
    "        n = f'{c}_d{k}'\n",
    "        train[n] = ((train[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        test[n] = ((test[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        orig[n] = ((orig[c] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "        DIGITS.append(n)\n",
    "\n",
    "train['grade_subgrade_d1'] = train['grade_subgrade'].apply(lambda x: x[1]).astype('int8')\n",
    "test['grade_subgrade_d1'] = test['grade_subgrade'].apply(lambda x: x[1]).astype('int8')\n",
    "orig['grade_subgrade_d1'] = orig['grade_subgrade'].apply(lambda x: x[1]).astype('int8')\n",
    "\n",
    "ROUND = []\n",
    "RR = [-1, 0]\n",
    "for c in ['annual_income', 'loan_amount']:\n",
    "    for r in RR:\n",
    "        n = f\"{c}_r{r}\"\n",
    "        train[n] = train[c].round(r)\n",
    "        test[n] = test[c].round(r)\n",
    "        orig[n] = orig[c].round(r)\n",
    "        ROUND.append(n)\n",
    "\n",
    "for c in CATS + ['credit_score2']:\n",
    "    combined = pd.concat([train[c], test[c], orig[c]])\n",
    "    combined, _ = combined.factorize()\n",
    "    train[c] = combined[:len(train)]\n",
    "    test[c] = combined[len(train):len(train) + len(test)]\n",
    "    orig[c] = combined[len(train) + len(test):]\n",
    "\n",
    "TE_columns = []\n",
    "CE_columns = []\n",
    "PAIRS = []\n",
    "\n",
    "columns = NUMS + CATS + [ROUND[0]]\n",
    "\n",
    "for r in [2]:\n",
    "    for cols in tqdm(list(combinations(columns, r))):\n",
    "        name = '-'.join(cols)\n",
    "\n",
    "        train[name] = train[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            train[name] = train[name] + '_' + train[col].astype(str)\n",
    "\n",
    "        test[name] = test[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            test[name] = test[name] + '_' + test[col].astype(str)\n",
    "\n",
    "        orig[name] = orig[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            orig[name] = orig[name] + '_' + orig[col].astype(str)\n",
    "\n",
    "        combined = pd.concat([train[name], test[name], orig[name]], ignore_index=True)\n",
    "        combined, _ = combined.factorize()\n",
    "        if pd.Series(combined).nunique() > len(combined) // 2:\n",
    "            train = train.drop(name, axis=1)\n",
    "            test = test.drop(name, axis=1)\n",
    "            orig = orig.drop(name, axis=1)\n",
    "            continue\n",
    "        train[name] = combined[:len(train)]\n",
    "        test[name] = combined[len(train):len(train) + len(test)]\n",
    "        orig[name] = combined[len(train) + len(test):]\n",
    "\n",
    "        TE_columns.append(name)\n",
    "        CE_columns.append(name)\n",
    "        PAIRS.append(name)\n",
    "\n",
    "for c1 in DIGITS[:6]:\n",
    "    for c2 in ['employment_status', 'debt_to_income_ratio']:\n",
    "        name = f'{c1}-{c2}'\n",
    "\n",
    "        train[name] = train[c1].astype(str) + '_' + train[c2].astype(str)\n",
    "        test[name] = test[c1].astype(str) + '_' + test[c2].astype(str)\n",
    "        orig[name] = orig[c1].astype(str) + '_' + orig[c2].astype(str)\n",
    "\n",
    "        combined = pd.concat([train[name], test[name], orig[name]], ignore_index=True)\n",
    "        combined, _ = combined.factorize()\n",
    "        train[name] = combined[:len(train)]\n",
    "        test[name] = combined[len(train):len(train) + len(test)]\n",
    "        orig[name] = combined[len(train) + len(test):]\n",
    "\n",
    "        TE_columns.append(name)\n",
    "\n",
    "for c1 in DIGITS[:6]:\n",
    "    for c2 in [DIGITS[6], DIGITS[7]]:\n",
    "        name = f'{c1}-{c2}'\n",
    "\n",
    "        train[name] = train[c1].astype(str) + '_' + train[c2].astype(str)\n",
    "        test[name] = test[c1].astype(str) + '_' + test[c2].astype(str)\n",
    "        orig[name] = orig[c1].astype(str) + '_' + orig[c2].astype(str)\n",
    "\n",
    "        combined = pd.concat([train[name], test[name], orig[name]], ignore_index=True)\n",
    "        combined, _ = combined.factorize()\n",
    "        train[name] = combined[:len(train)]\n",
    "        test[name] = combined[len(train):len(train) + len(test)]\n",
    "        orig[name] = combined[len(train) + len(test):]\n",
    "\n",
    "        TE_columns.append(name)\n",
    "\n",
    "for cols in tqdm([['annual_income', 'gender', 'marital_status']]):\n",
    "    name = '-'.join(cols)\n",
    "\n",
    "    train[name] = train[cols[0]].astype(str)\n",
    "    for col in cols[1:]:\n",
    "        train[name] = train[name] + '_' + train[col].astype(str)\n",
    "\n",
    "    test[name] = test[cols[0]].astype(str)\n",
    "    for col in cols[1:]:\n",
    "        test[name] = test[name] + '_' + test[col].astype(str)\n",
    "\n",
    "    orig[name] = orig[cols[0]].astype(str)\n",
    "    for col in cols[1:]:\n",
    "        orig[name] = orig[name] + '_' + orig[col].astype(str)\n",
    "\n",
    "    combined = pd.concat([train[name], test[name], orig[name]], ignore_index=True)\n",
    "    combined, _ = combined.factorize()\n",
    "    train[name] = combined[:len(train)]\n",
    "    test[name] = combined[len(train):len(train) + len(test)]\n",
    "    orig[name] = combined[len(train) + len(test):]\n",
    "\n",
    "    TE_columns.append(name)\n",
    "\n",
    "TE_ORIG = []\n",
    "CC = CATS + NUMS + DIGITS[:16]\n",
    "\n",
    "print(f\"Processing {len(CC)} columns... \",end=\"\")\n",
    "for i,c in enumerate(CC):\n",
    "    if i%10==0: print(f\"{i}, \",end=\"\")\n",
    "    tmp = orig.groupby(c)[TARGET].mean()\n",
    "    tmp = tmp.astype('float32')\n",
    "    tmp.name = f\"TE_ORIG_{c}\"\n",
    "    TE_ORIG.append( f\"TE_ORIG_{c}\" )\n",
    "    train = train.merge(tmp, on=c, how='left')\n",
    "    train[tmp.name] = train[tmp.name].fillna(train[tmp.name].mean())\n",
    "    test = test.merge(tmp, on=c, how='left')\n",
    "    test[tmp.name] = test[tmp.name].fillna(train[tmp.name].mean())\n",
    "\n",
    "    tmp = orig[c].value_counts()\n",
    "    TE_ORIG.append( f\"CE_ORIG_{c}\" )\n",
    "    train[f'CE_ORIG_{c}'] = train[c].map(tmp).fillna(0)\n",
    "    test[f'CE_ORIG_{c}'] = test[c].map(tmp).fillna(0)\n",
    "print()\n",
    "\n",
    "CC = CATS + NUMS\n",
    "\n",
    "print(f\"Processing {len(CC)} columns... \",end=\"\")\n",
    "for i,c in enumerate(CC):\n",
    "    if i%10==0: print(f\"{i}, \",end=\"\")\n",
    "    tmp = orig.groupby(c)['employment_status'].mean()\n",
    "    tmp = tmp.astype('float32')\n",
    "    tmp.name = f\"TE_ORIG_(employment_status)_{c}\"\n",
    "    TE_ORIG.append( f\"TE_ORIG_(employment_status)_{c}\" )\n",
    "    train = train.merge(tmp, on=c, how='left')\n",
    "    train[tmp.name] = train[tmp.name].fillna(train[tmp.name].mean())\n",
    "    test = test.merge(tmp, on=c, how='left')\n",
    "    test[tmp.name] = test[tmp.name].fillna(train[tmp.name].mean())\n",
    "\n",
    "print()\n",
    "\n",
    "DIGIT_PAIRS = []\n",
    "for r in [2, 3, 4]:\n",
    "    for cols in tqdm(list(combinations(DIGITS[:6], r))):\n",
    "        name = '-'.join(cols)\n",
    "\n",
    "        train[name] = train[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            train[name] = train[name] + '_' + train[col].astype(str)\n",
    "\n",
    "        test[name] = test[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            test[name] = test[name] + '_' + test[col].astype(str)\n",
    "\n",
    "        orig[name] = orig[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            orig[name] = orig[name] + '_' + orig[col].astype(str)\n",
    "\n",
    "        combined = pd.concat([train[name], test[name], orig[name]], ignore_index=True)\n",
    "        combined, _ = combined.factorize()\n",
    "        train[name] = combined[:len(train)]\n",
    "        test[name] = combined[len(train):len(train) + len(test)]\n",
    "        orig[name] = combined[len(train) + len(test):]\n",
    "\n",
    "        DIGIT_PAIRS.append(name)\n",
    "        TE_columns.append(name)\n",
    "        CE_columns.append(name)\n",
    "\n",
    "for r in [2, 3, 4]:\n",
    "    for cols in tqdm(list(combinations(DIGITS[6:12], r))):\n",
    "        name = '-'.join(cols)\n",
    "\n",
    "        train[name] = train[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            train[name] = train[name] + '_' + train[col].astype(str)\n",
    "\n",
    "        test[name] = test[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            test[name] = test[name] + '_' + test[col].astype(str)\n",
    "\n",
    "        orig[name] = orig[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            orig[name] = orig[name] + '_' + orig[col].astype(str)\n",
    "\n",
    "        combined = pd.concat([train[name], test[name], orig[name]], ignore_index=True)\n",
    "        combined, _ = combined.factorize()\n",
    "        train[name] = combined[:len(train)]\n",
    "        test[name] = combined[len(train):len(train) + len(test)]\n",
    "        orig[name] = combined[len(train) + len(test):]\n",
    "\n",
    "        DIGIT_PAIRS.append(name)\n",
    "        TE_columns.append(name)\n",
    "        CE_columns.append(name)\n",
    "\n",
    "for r in [2, 3, 4]:\n",
    "    for cols in tqdm(list(combinations(DIGITS[12:16], r))):\n",
    "        name = '-'.join(cols)\n",
    "\n",
    "        train[name] = train[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            train[name] = train[name] + '_' + train[col].astype(str)\n",
    "\n",
    "        test[name] = test[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            test[name] = test[name] + '_' + test[col].astype(str)\n",
    "\n",
    "        orig[name] = orig[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            orig[name] = orig[name] + '_' + orig[col].astype(str)\n",
    "\n",
    "        combined = pd.concat([train[name], test[name], orig[name]], ignore_index=True)\n",
    "        combined, _ = combined.factorize()\n",
    "        train[name] = combined[:len(train)]\n",
    "        test[name] = combined[len(train):len(train) + len(test)]\n",
    "        orig[name] = combined[len(train) + len(test):]\n",
    "\n",
    "        DIGIT_PAIRS.append(name)\n",
    "        TE_columns.append(name)\n",
    "        CE_columns.append(name)\n",
    "\n",
    "for r in [2, 3]:\n",
    "    for cols in tqdm(list(combinations(DIGITS[16:19], r))):\n",
    "        name = '-'.join(cols)\n",
    "\n",
    "        train[name] = train[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            train[name] = train[name] + '_' + train[col].astype(str)\n",
    "\n",
    "        test[name] = test[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            test[name] = test[name] + '_' + test[col].astype(str)\n",
    "\n",
    "        orig[name] = orig[cols[0]].astype(str)\n",
    "        for col in cols[1:]:\n",
    "            orig[name] = orig[name] + '_' + orig[col].astype(str)\n",
    "\n",
    "        combined = pd.concat([train[name], test[name], orig[name]], ignore_index=True)\n",
    "        combined, _ = combined.factorize()\n",
    "        train[name] = combined[:len(train)]\n",
    "        test[name] = combined[len(train):len(train) + len(test)]\n",
    "        orig[name] = combined[len(train) + len(test):]\n",
    "\n",
    "        DIGIT_PAIRS.append(name)\n",
    "        TE_columns.append(name)\n",
    "        CE_columns.append(name)\n",
    "\n",
    "for c in NUMS + CATS:\n",
    "    if c != 'employment_status':\n",
    "        tmp = train.groupby(c)['employment_status'].mean()\n",
    "        tmp.name = f'TE_mean_(employment_status)_{c}'\n",
    "        train = train.merge(tmp, on=c, how='left')\n",
    "        train[tmp.name] = train[tmp.name].fillna(train[tmp.name].mean())\n",
    "        test = test.merge(tmp, on=c, how='left')\n",
    "        test[tmp.name] = test[tmp.name].fillna(train[tmp.name].mean())\n",
    "\n",
    "for c in NUMS + CATS:\n",
    "    if c != 'debt_to_income_ratio':\n",
    "        tmp = train.groupby(c)['debt_to_income_ratio'].mean()\n",
    "        tmp.name = f'TE_mean_(debt_to_income_ratio)_{c}'\n",
    "        train = train.merge(tmp, on=c, how='left')\n",
    "        train[tmp.name] = train[tmp.name].fillna(train[tmp.name].mean())\n",
    "        test = test.merge(tmp, on=c, how='left')\n",
    "        test[tmp.name] = test[tmp.name].fillna(train[tmp.name].mean())\n",
    "\n",
    "for c in test.columns.tolist():\n",
    "    if test[c].dtype == 'float64':\n",
    "        train[c] = train[c].astype('float32')\n",
    "        test[c] = test[c].astype('float32')\n",
    "    if test[c].dtype == 'int64':\n",
    "        train[c] = train[c].astype('int32')\n",
    "        test[c] = test[c].astype('int32')\n",
    "\n",
    "FEATURES = train.columns.tolist()\n",
    "FEATURES.remove(TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T04:40:49.724593Z",
     "iopub.status.busy": "2025-11-23T04:40:49.724224Z",
     "iopub.status.idle": "2025-11-23T04:40:49.729065Z",
     "shell.execute_reply": "2025-11-23T04:40:49.728364Z",
     "shell.execute_reply.started": "2025-11-23T04:40:49.724569Z"
    },
    "id": "5L236nXjGs6g",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_encode(train, valid, test, col):\n",
    "    counts = train[col].value_counts()\n",
    "\n",
    "    train[f'CE_{col}'] = train[col].map(counts)\n",
    "    valid[f'CE_{col}'] = valid[col].map(counts).fillna(0)\n",
    "    test[f'CE_{col}'] = test[col].map(counts).fillna(0)\n",
    "    return (train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T04:48:57.105906Z",
     "iopub.status.busy": "2025-11-23T04:48:57.105616Z",
     "iopub.status.idle": "2025-11-23T05:28:50.508836Z",
     "shell.execute_reply": "2025-11-23T05:28:50.508115Z",
     "shell.execute_reply.started": "2025-11-23T04:48:57.105885Z"
    },
    "id": "KbjzYgc_Gs6h",
    "outputId": "fb159683-5fd0-45f0-ea1d-77389ecb0bd9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "oof = np.zeros(len(train))\n",
    "pred = np.zeros(len(test))\n",
    "models = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=20, random_state=42, shuffle=True)\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(train)), train[TARGET])):\n",
    "    X_train, X_val = train.loc[train_idx, FEATURES], train.loc[val_idx, FEATURES]\n",
    "    y_train, y_val = train.loc[train_idx, TARGET], train.loc[val_idx, TARGET]\n",
    "    X_test = test.copy()\n",
    "\n",
    "    for col in tqdm(CE_columns):\n",
    "        X_train, X_val, X_test = count_encode(X_train, X_val, X_test, col)\n",
    "\n",
    "    for col in tqdm(TE_columns + ['credit_score2']):\n",
    "        encoder = TargetEncoder(cv=10, random_state=42, shuffle=True)\n",
    "        X_train[col] = encoder.fit_transform(pd.DataFrame(X_train[col]), y_train).flatten()\n",
    "        X_val[col] = encoder.transform(pd.DataFrame(X_val[col])).flatten()\n",
    "        X_test[col] = encoder.transform(pd.DataFrame(X_test[col])).flatten()\n",
    "\n",
    "    for col in tqdm(ROUND):\n",
    "        n = f'TE_mean_{col}'\n",
    "        encoder = TargetEncoder(cv=10, random_state=42, shuffle=True)\n",
    "        X_train[n] = encoder.fit_transform(pd.DataFrame(X_train[col]), y_train).flatten()\n",
    "        X_val[n] = encoder.transform(pd.DataFrame(X_val[col])).flatten()\n",
    "        X_test[n] = encoder.transform(pd.DataFrame(X_test[col])).flatten()\n",
    "\n",
    "    agg_list = ['mean', 'std', 'min', 'max']\n",
    "\n",
    "    for agg in agg_list:\n",
    "        X_train[f'TE_{agg}'] = X_train[TE_columns].agg(agg, axis=1)\n",
    "        X_val[f'TE_{agg}'] = X_val[TE_columns].agg(agg, axis=1)\n",
    "        X_test[f'TE_{agg}'] = X_test[TE_columns].agg(agg, axis=1)\n",
    "\n",
    "    for c in X_train.columns.tolist():\n",
    "        if X_train[c].dtype == 'float64':\n",
    "            X_train[c] = X_train[c].astype('float32')\n",
    "            X_val[c] = X_val[c].astype('float32')\n",
    "            X_test[c] = X_test[c].astype('float32')\n",
    "        if X_train[c].dtype == 'int64':\n",
    "            X_train[c] = X_train[c].astype('int32')\n",
    "            X_val[c] = X_val[c].astype('int32')\n",
    "            X_test[c] = X_test[c].astype('int32')\n",
    "\n",
    "    combined = pd.concat([X_train, X_val, X_test])\n",
    "    combined[DIGITS] = combined[DIGITS].astype('category')\n",
    "    X_train = combined[:len(X_train)]\n",
    "    X_val = combined[len(X_train):len(X_train) + len(X_val)]\n",
    "    X_test = combined[len(X_train) + len(X_val):]\n",
    "    del combined\n",
    "    gc.collect()\n",
    "\n",
    "    param_grid = {'colsample_bytree': 0.2364,\n",
    "                  'gamma': 0.034283,\n",
    "                  'max_depth': 6,\n",
    "                  'reg_alpha': 0.71367,\n",
    "                  'reg_lambda': 4.43564,\n",
    "                  'subsample': 0.59394}\n",
    "\n",
    "    model = XGBClassifier(**param_grid,\n",
    "                          n_estimators=10000,\n",
    "                          objective='binary:logistic',\n",
    "                          eval_metric='auc',\n",
    "                          learning_rate=0.01,\n",
    "                          early_stopping_rounds=500,\n",
    "                          max_bin=1024,\n",
    "                          random_state=42,\n",
    "                          enable_categorical=True,\n",
    "                          device='cuda',\n",
    "                          n_jobs=-1)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=100)\n",
    "    oof[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    pred += model.predict_proba(X_test)[:, 1]\n",
    "    print(f'Fold {idx + 1}: {roc_auc_score(y_val, oof[val_idx])}', flush=True)\n",
    "\n",
    "    models.append(model)\n",
    "    del X_train, X_val, y_train, y_val, model\n",
    "    gc.collect()\n",
    "\n",
    "pred /= 5\n",
    "print(f'CV AUC: {roc_auc_score(train[TARGET], oof)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMIIJso-Gs6h",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\n",
    "submission[TARGET] = pred\n",
    "submission.to_csv('xgb.csv', index=False)\n",
    "pd.DataFrame({'xgb_oof': oof}).to_csv('xgb_oof.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14262372,
     "sourceId": 91722,
     "sourceType": "competition"
    },
    {
     "datasetId": 8264672,
     "sourceId": 13059803,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
